{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Approaches \n",
    "##### 1 - Using mediapipe to get hand coordinates and then using these as features to train the model and get classes\n",
    "##### 2 - Without mediapipe directly training the images and getting output as different classes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Images: (800, 224, 224) Labels: (800,)\n",
      "Validation set - Images: (200, 224, 224) Labels: (200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Resize\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Path to the directory\n",
    "directory_path = \"D:/sign_language_recognition/dataset sign language/\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "resize_transform = Resize((224, 224))\n",
    "\n",
    "for root, dirs, files in os.walk(directory_path):\n",
    "    if root == directory_path:\n",
    "        continue\n",
    "\n",
    "    label = int(os.path.basename(root))\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            # Load the image\n",
    "            image_path = os.path.join(root, file)\n",
    "            image = Image.open(image_path)\n",
    "            image = image.convert(\"L\")\n",
    "\n",
    "            # Resize the image\n",
    "            resized_image = resize_transform(image)\n",
    "\n",
    "            # Append the image and label to the arrays\n",
    "            images.append(resized_image)\n",
    "            labels.append(label)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the arrays\n",
    "print(\"Training set - Images:\", X_train.shape, \"Labels:\", y_train.shape)\n",
    "print(\"Validation set - Images:\", X_val.shape, \"Labels:\", y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)  # Assuming grayscale images\n",
    "X_train = np.repeat(X_train, 3, axis=-1)  # Repeat the single channel to match the expected 3 channels\n",
    "\n",
    "X_val = np.expand_dims(X_val, axis=-1)  # Assuming grayscale images\n",
    "X_val = np.repeat(X_val, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "\n",
    "# Load pre-trained ResNet50 model without the top (classification) layer\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# Load pre-trained EfficientNetB0 model without the top (classification) layer\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new classification layers\n",
    "flatten_layer = layers.Flatten()(base_model.output)\n",
    "dense_layer = layers.Dense(512, activation='relu')(flatten_layer)\n",
    "output_layer = layers.Dense(10, activation='softmax')(dense_layer)  # Assuming 10 classes for the output\n",
    "\n",
    "# Combine the base model with custom classification layers\n",
    "model = models.Model(inputs=base_model.input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 10 classes.\n",
      "Found 200 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Create an ImageDataGenerator with augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Flow from directory with augmentation\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'D:/sign_language_recognition/dataset sign language/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Flow from directory without augmentation (for validation)\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    'D:/sign_language_recognition/dataset sign language/valid',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 8s 321ms/step - loss: 3.3773 - accuracy: 0.8575 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 8s 321ms/step - loss: 1.5361 - accuracy: 0.9175 - val_loss: 0.4127 - val_accuracy: 0.9900\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 8s 324ms/step - loss: 1.5508 - accuracy: 0.9312 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 8s 322ms/step - loss: 0.9063 - accuracy: 0.9575 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 8s 320ms/step - loss: 0.6526 - accuracy: 0.9675 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 8s 323ms/step - loss: 0.6434 - accuracy: 0.9638 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 8s 323ms/step - loss: 0.5308 - accuracy: 0.9700 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 8s 327ms/step - loss: 0.4634 - accuracy: 0.9750 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 8s 326ms/step - loss: 0.4555 - accuracy: 0.9762 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 8s 322ms/step - loss: 0.5271 - accuracy: 0.9737 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c1c3cfb2e0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use these generators for training\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "Prediction: [[4.1400328e-09 9.9204820e-01 2.1029218e-32 3.1539818e-10 7.5203981e-03\n",
      "  5.8461647e-15 3.7913790e-04 1.3975905e-34 5.2328538e-05 1.4788157e-19]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Load the test image using OpenCV\n",
    "test_image = cv2.imread(\"C:/Users/pytho/Desktop/maxresdefault.jpg\")\n",
    "# Resize the test image\n",
    "resized_test_image = cv2.resize(test_image, (224, 224))  # Resize to model's input shape\n",
    "# Preprocess the input image\n",
    "preprocessed_test_image = preprocess_input(resized_test_image)\n",
    "\n",
    "# Reshape the image for prediction (add batch dimension)\n",
    "input_image = preprocessed_test_image.reshape((1,) + preprocessed_test_image.shape)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(input_image)\n",
    "\n",
    "print(\"Prediction:\", prediction)\n",
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# Initialize the camera\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    frame1 = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    frame_resized = cv2.resize(frame1, (224, 224))  # Resize to model's input shape\n",
    "    frame_for_prediction = np.expand_dims(frame_resized, axis=0)  # Add batch dimension\n",
    "    #frame_for_prediction = preprocess_input(frame_for_prediction)  # Preprocess input for EfficientNet\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(frame_for_prediction)\n",
    "\n",
    "    # Resize frame back to 1024x1024 for display\n",
    "    frame_display = cv2.resize(frame1, (1024, 1024))\n",
    "\n",
    "    # Convert frame back to BGR for display\n",
    "    frame_display_bgr = cv2.cvtColor(frame_display, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Display the frame and prediction\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.putText(frame, 'Prediction: {}'.format(prediction), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
